{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zriTdjauH8iQ"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers huggingface_hub\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmTTyjUGqol"
      },
      "source": [
        "### Build-a-transformer\n",
        "\n",
        "In this section, you will implement a transformer language model layer by layer, then use it to generate (hopefully) coherent text.\n",
        "\n",
        "To understand how these layers work, please check out our guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
        "\n",
        "\n",
        "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) - a prominent model from 2019.\n",
        "\n",
        "\n",
        "\n",
        "Idea & code by: Ilya Beletsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "d929942bb0d64370a36dff733a7aa41a",
            "5a0da015885f4e729709cd18e1770764",
            "1bda91588f9f4b09a6fb0ea5eec65db2",
            "df38878ef1be45d29d37189a9344a577",
            "912d16d935fe4077883210db847eeb74",
            "39e31b84a4a44e038050ef8761eb18d8",
            "71b6b62f9cef409eab9e8496a27d7d45",
            "7efceed1649a43baaaf91b4295947a0a",
            "fea7de1bb6d745f3b67aa17d0b91b990",
            "42e100c6b4174994b07e50b38e9ff5a5",
            "e769f130b7904c9a8e8f6acc09e7b2cd"
          ]
        },
        "id": "vOcK0lGTGqol",
        "outputId": "712bdaa8-3a5d-431b-c843-bba7deb26eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d929942bb0d64370a36dff733a7aa41a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
        "for key, value in tuple(state_dict.items()):\n",
        "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
        "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
        "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
        "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
        "\n",
        "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0SUtQnGqom"
      },
      "source": [
        "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
        "\n",
        "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
        "\n",
        "The fully connected layers are by far easier to understand, so we shall begin there:\n",
        "\n",
        "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rh-6DX9Gqom"
      },
      "outputs": [],
      "source": [
        "class GeLUThatWasUsedInGPT2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
        "        self.gelu = GeLUThatWasUsedInGPT2()\n",
        "        self.c_proj = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVGKnHBGqom"
      },
      "source": [
        "Now, let's test that it works with GPT-2 weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoWjZwZkGqom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d757b6fb-abef-4cef-89f7-34aaee88bd76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems legit!\n"
          ]
        }
      ],
      "source": [
        "mlp = FullyConnected(dim=768)\n",
        "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
        "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
        "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
        "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(mlp(x) * x)\n",
        "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
        "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
        "print(\"Seems legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbfCevRwGqom"
      },
      "source": [
        "Now, let's get to attention layers.\n",
        "\n",
        "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
        "\n",
        "As before, please implement masked self-attention __without layernorm or residual connections.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6j7M4hLGqon"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "\n",
        "        # Note: this is an inefficient implementation that uses a for-loop.\n",
        "        # To get the full grade during homework, please re-implement this code:\n",
        "        # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
        "        # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
        "        head_outputs = []\n",
        "        for head_index in range(self.num_heads):\n",
        "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
        "\n",
        "            head_queries = q[..., head_selector]\n",
        "            head_keys = k[..., head_selector]\n",
        "            head_values = v[..., head_selector]\n",
        "            single_head_output = F.scaled_dot_product_attention(\n",
        "                head_queries,\n",
        "                head_keys,\n",
        "                head_values,\n",
        "                is_causal=True\n",
        "            )\n",
        "            # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "            head_outputs.append(single_head_output)\n",
        "\n",
        "        combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
        "        return self.c_proj(combined_head_outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umZpcpIkJva7"
      },
      "source": [
        "Test that it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg5Oj_PPM6hj",
        "outputId": "30b706e4-cd93-4cec-cb82-5b9e393ddc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It works!\n"
          ]
        }
      ],
      "source": [
        "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
        "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
        "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
        "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
        "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(attn(x) * x)\n",
        "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
        "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
        "print(\"It works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn6tgTHzOK4l"
      },
      "source": [
        "We can now combine attention and MLP to build the full transformer layer:\n",
        "\n",
        "![img](https://i.imgur.com/1sq2vHO.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3AH7YQvRpvU"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FullyConnected(dim)\n",
        "    def forward(self, x):\n",
        "        #YOUR CODE - apply attention, mlp and layer normalization as shown in figure above\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzo_QeFVSNZa",
        "outputId": "a277a256-618f-4cd4-bf48-8c30c1acfb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "layer = TransformerLayer(dim=768, num_heads=12)\n",
        "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
        "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbqw9iuaSrYy"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
        "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
        "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
        "\n",
        "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        token_embeddings = self.wte(input_ids)\n",
        "        position_embeddings = self.wpe(position_ids)\n",
        "        full_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "        transformer_output = self.h(full_embeddings)\n",
        "        transformer_output_ln = self.ln_f(transformer_output)\n",
        "\n",
        "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
        "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0m8jt66aDIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "fccb5739882b4f108d835976ab0e31d7",
            "a0c99bfdee6149fda2f33d9a85e83c4b",
            "27780d1fb7a0453f907f8f401ffb3c54",
            "4e46e8485fdd4dce8d56655e45fa4f5f",
            "f23ffac40aac481a98f1f30209a15419",
            "2b50ab9f546f4c73ac45a8a79e00d029",
            "561d6197c85e4812a3e80fcf88ea2fca",
            "7bcf6c5b5bda4c5d8ff17268f5b0ac5d",
            "a9e810747ba4411da68604358d921ce7",
            "46b89c45f2ac475599b98808c9f11401",
            "1bbb2e58128e47a997b5891923bd5d89",
            "45504c2c5da945c79dea4269c88a496a",
            "3c11e07283eb46c7b0821fae63749949",
            "ab15d81a3ec04532b51056baed652773",
            "0efc93c7e8a64733ad043ef091697e7d",
            "dc413ca08bc34b55b072c3200aea56f8",
            "627933ea286448b6b4102067f3e8dd29",
            "41a50818453d4a43931702e0544337bd",
            "b939ed948c554b3081a202d64b56abd5",
            "4cc4befb562b42e788b3ba75a4c524d1",
            "4b3bfdf3fd2544a09dd57bf709a49b06",
            "396f21e2ac234fe789e0d7329e1cb80b",
            "be11b43a4bff455c9174855c20266982",
            "89f0ebcfe76d4a2c8dc46708073da113",
            "09c35129d2eb410b9df7f58583e70f04",
            "4c792aa9b4394481a34b5de6f8a35ee8",
            "b411bdaf8e6742e6b2067230025d8ce7",
            "88c2ee8212ec4d0f9ac2ee86efde05b2",
            "3ed5f383e5b84c3fb0b50433c8c80bc0",
            "05f8eacf25744ca4a64f82cf593bed16",
            "e6ee1204760d4ec2b3fb91d9db210b5b",
            "af6651d479a64715972234101059f958",
            "8e13909310984a039c7c3b3a4421ce26",
            "cac00bb415464902b6878e05a9c55456",
            "030773b5a2f74ab895ad467064124de1",
            "589fad78d6f94adc9a388065a41dfb28",
            "e3f50c1e7d594e5a9c19e599f337dedc",
            "2229295ae394439f92950d057bf2ca6b",
            "7582981e5ca64380b4d86bed2273afcf",
            "30be0c66fe8f41b2be019f77264ab196",
            "792f7354feaf4d11be0bef0a6515e09a",
            "1687636bbb8341a5aa00b01f41e393f6",
            "23403e9bfa4c4d2a8dbee3f9c9495545",
            "843cd81e875f43d284f47f49779fa7f5",
            "e8a5b869297d4bc0b816c8751f19db00",
            "4fb124ce779f48498175961e00af9d28",
            "fb4dcd4c77de432686abf84ea67f6c07",
            "ff74dccfbc4143ab876cb59d51bda555",
            "f23b922b7252455b828aba74805972a7",
            "da7e09ddea854512a08313b7b6aae26e",
            "99d0a9a0365045a1864570b7d86c1940",
            "80317eaa3c874c42992cffe76f9a8bd4",
            "6098328a0226471588d13a4d45f01e67",
            "70285c1df7884cdb96c1aa839a822a52",
            "ce491ea59686470bb8d16a05bdca606f"
          ]
        },
        "outputId": "6e98059f-dabc-4e58-a61e-a6c0535af864"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fccb5739882b4f108d835976ab0e31d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45504c2c5da945c79dea4269c88a496a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be11b43a4bff455c9174855c20266982"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cac00bb415464902b6878e05a9c55456"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8a5b869297d4bc0b816c8751f19db00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  look\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
        "\n",
        "predicted_logits = model(input_ids)\n",
        "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
        "\n",
        "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ql3Lo7dXZ2",
        "outputId": "8e81d803-8da6-44c0-c106-81c4cdd2e4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Fermi paradox  : a complex notion that quantum mechanics holds as real is the object of long, thirsty debate. Comments\n",
            " (no more!) from Dr. David Smith in The Federalist (opens with some comment bar): I agree that there are three key problems\n",
            " with any kind of evolution theory. These are important in ways that motivated James Clerk Maxwell to create the 1969 Quantum\n",
            " Theory . All three questions also involve Barty, Vogel & Darwin's (albeit incredibly primitive) in-between Principle serves\n",
            " as accommodation for linear ontology. According to these related notions, quantum mechanics is intuitional evolution and\n",
            " (non-linear) matter does not. After my Googling I found another interesting quark on Wikipedia (at this point having tame\n",
            "ly identified it). Though fully aware that most who discuss new concepts have encountered both the slow growth of this constantly\n",
            " displaced origamiist myth and the speed and psychology of quantum physics we can probably agree that Barty and Vogel are\n",
            " both sufficient shorthand for what has been previously conceptualized as natural selection. Finally with respect to my favorite\n",
            " quark that always turns off and the 'peanutslam hypothesis', I do believe they are the most sensible. Many common ideas cling\n",
            " to the principles of the Baryoid paradox, momentum and  the explicit nature of thermodynamics . This idea is mere another\n",
            " way of saying common urge, mere 'opportunity ' for new convergent behaviour from being 'similar' to 'self-similar'. If Bart\n",
            "y and Vogel had bremer chaos and can385 fully explain why there is a mere 1,040 variables, mechanics would explain the entire\n",
            " universe in approximately 300 billion years. Manifold and quantum theory must therefore be fundamental to Newtonian physics\n",
            ", or more accurately, it is about our causal sense to make meaningful transitions between different dimensions onwards to\n",
            " good capacity for the underliements it is finitely powerful. This saying weighs on Monke III again now that Burke is back\n",
            ". He claims we need infinite time to be on the same wavelength of extension from Craig's (1992b) Wilson footprint (talk) \n",
            " (parameters of mass and energy are multiple of each other's measurement frequencies, hence it doesn't distinguish between\n",
            " metafunctions that contradict (as already stated) th all g different measurements among of another. But from citations (\n",
            "no more!) from Robert Salter (£45 minimum, magazine pays about £50 a cap) I'm ready to take our rubbish on Batty Jim and now\n",
            " Vince are writing right through  Thomas"
          ]
        }
      ],
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens]))\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTOHu124Gqop",
        "outputId": "46a6e71a-4554-45f2-f22f-f285ca928f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "9ee5aa556b654d50acd045bb545e5f7e",
            "2b76fa9cad71490b8dfccccb7bc929da",
            "df8d91965be844a4aae5cadf899371ce",
            "7b61d85d3b284ab1bba88c4f97342bdb",
            "fc48366261cd45558b2a4b5293cce363",
            "2754f50e6b4d419698270db2a8400b9e",
            "4d8649376f5b40e3a1c39c33bac30ff5",
            "5ec33a58e495439eab9d2535fe72c049",
            "4c985f6c896e48459e9950f739685134",
            "eb35ec0a3bb64986b80bc45e649ac455",
            "839bcef735384bdd93ea4a084b0642b2",
            "47c27a69d8eb44319965a5673f4482a7",
            "8915e2d988ad43bbadea389a45b8779c",
            "d95407fca6f345a889f1544537e4162b",
            "c9cbbc28978340fab2b772adb753b3f0",
            "7e624c049dd54d64961bea598c77fb53",
            "d1a83a43df0247ebb779c0c4c26cb32a",
            "4f3af9499a2f456fa546fd55aa844a84",
            "17ff04432b3e4c468ed0bff608a100d3",
            "61e2b298fa064e41afa93a788d104a32",
            "db99e124c64b4e95af59cf00ca336576",
            "eeefcb2b0c0f4eabbc13add1c2cf4d61"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ee5aa556b654d50acd045bb545e5f7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47c27a69d8eb44319965a5673f4482a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:  The Fermi paradox  is perhaps the least interesting and least understood phenomena of the Universe .  Because it is a very small world, it cannot be considered a complete vacuum, and so it is not a vacuum we could find with some simple telescopes.  It\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print('Generated text:', tokenizer.decode(\n",
        "    model.generate(\n",
        "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
        "        do_sample=True, max_new_tokens=50\n",
        "    ).flatten().numpy()\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWe0tYlQ3dQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}